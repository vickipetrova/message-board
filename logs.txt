* 
* ==> Audit <==
* |------------|------------------------|----------|------------------|---------|---------------------|---------------------|
|  Command   |          Args          | Profile  |       User       | Version |     Start Time      |      End Time       |
|------------|------------------------|----------|------------------|---------|---------------------|---------------------|
| start      |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:32 EET | 07 Nov 23 21:34 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:36 EET | 07 Nov 23 21:36 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:37 EET |                     |
| service    | flask-service          | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:37 EET | 07 Nov 23 21:40 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:44 EET | 07 Nov 23 21:44 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:44 EET | 07 Nov 23 21:44 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:46 EET | 07 Nov 23 21:46 EET |
| service    | flask-service          | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:48 EET | 07 Nov 23 21:53 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:48 EET | 07 Nov 23 21:48 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:49 EET | 07 Nov 23 21:49 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:51 EET | 07 Nov 23 21:51 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:52 EET | 07 Nov 23 21:52 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:53 EET | 07 Nov 23 21:53 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:54 EET | 07 Nov 23 21:54 EET |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 07 Nov 23 21:55 EET | 07 Nov 23 21:55 EET |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:43 EET | 10 Dec 23 22:45 EET |
| kubectl    | -- get pods -A         | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:48 EET | 10 Dec 23 22:48 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:49 EET | 10 Dec 23 22:49 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:50 EET |                     |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:57 EET | 10 Dec 23 22:57 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:58 EET |                     |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 22:59 EET | 10 Dec 23 22:59 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:08 EET | 10 Dec 23 23:08 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:13 EET | 10 Dec 23 23:13 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:14 EET | 10 Dec 23 23:14 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:14 EET |                     |
| service    | flask-service          | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:16 EET |                     |
| ip         |                        | minikube | victoria.petrova | v1.31.2 | 10 Dec 23 23:19 EET | 10 Dec 23 23:19 EET |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 11:59 EET | 12 Dec 23 12:01 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:01 EET |                     |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:02 EET | 12 Dec 23 12:03 EET |
| service    | kanban-app-service     | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:05 EET | 12 Dec 23 12:09 EET |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:11 EET | 12 Dec 23 12:13 EET |
| addons     | enable metrics-server  | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:13 EET | 12 Dec 23 12:13 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:14 EET |                     |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:15 EET | 12 Dec 23 12:15 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:15 EET |                     |
| service    | kanban-app-service     | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:16 EET |                     |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:17 EET | 12 Dec 23 12:17 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:18 EET | 12 Dec 23 12:18 EET |
| service    | kanban-app-service     | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:18 EET |                     |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:18 EET |                     |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 12:59 EET | 12 Dec 23 13:01 EET |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:03 EET | 12 Dec 23 13:03 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:03 EET |                     |
| addons     | disable metrics-server | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:04 EET | 12 Dec 23 13:04 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:04 EET |                     |
| addons     | enable metrics-server  | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:05 EET | 12 Dec 23 13:05 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:06 EET |                     |
| stop       |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:08 EET | 12 Dec 23 13:08 EET |
| addons     | disable metrics-server | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:09 EET | 12 Dec 23 13:09 EET |
| start      |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:09 EET | 12 Dec 23 13:09 EET |
| dashboard  |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:10 EET |                     |
| docker-env |                        | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:10 EET | 12 Dec 23 13:10 EET |
| service    | kanban-app-service     | minikube | victoria.petrova | v1.31.2 | 12 Dec 23 13:10 EET |                     |
|------------|------------------------|----------|------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/12 13:09:12
Running on machine: Victorias-MacBook-Pro
Binary: Built with gc go1.20.7 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1212 13:09:12.057567   51061 out.go:296] Setting OutFile to fd 1 ...
I1212 13:09:12.057904   51061 out.go:348] isatty.IsTerminal(1) = true
I1212 13:09:12.057907   51061 out.go:309] Setting ErrFile to fd 2...
I1212 13:09:12.057912   51061 out.go:348] isatty.IsTerminal(2) = true
I1212 13:09:12.058101   51061 root.go:338] Updating PATH: /Users/victoria.petrova/.minikube/bin
W1212 13:09:12.058203   51061 root.go:314] Error reading config file at /Users/victoria.petrova/.minikube/config/config.json: open /Users/victoria.petrova/.minikube/config/config.json: no such file or directory
I1212 13:09:12.060910   51061 out.go:303] Setting JSON to false
I1212 13:09:12.105023   51061 start.go:128] hostinfo: {"hostname":"Victorias-MacBook-Pro.local","uptime":2996155,"bootTime":1699383197,"procs":734,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.2.1","kernelVersion":"22.3.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"9d253e7b-5111-5cca-94e5-cbc49e58e474"}
W1212 13:09:12.105178   51061 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1212 13:09:12.127212   51061 out.go:177] 😄  minikube v1.31.2 on Darwin 13.2.1
I1212 13:09:12.167197   51061 notify.go:220] Checking for updates...
I1212 13:09:12.168503   51061 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1212 13:09:12.169124   51061 driver.go:373] Setting default libvirt URI to qemu:///system
I1212 13:09:12.267328   51061 docker.go:121] docker version: linux-20.10.21:Docker Desktop 4.14.0 (91374)
I1212 13:09:12.268667   51061 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1212 13:09:13.973801   51061 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.705110638s)
I1212 13:09:13.977483   51061 info.go:266] docker info: {ID:3Y6K:F5AV:56CJ:JDK5:HMTQ:22JX:GZA2:5L73:MPQZ:UNBT:FBKF:JP57 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:false NGoroutines:85 SystemTime:2023-12-12 11:09:12.333988773 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8241238016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr:192.168.65.3 LocalNodeState:error ControlAvailable:false Error:error while loading TLS certificate in /var/lib/docker/swarm/certificates/swarm-node.crt: certificate (1 - sbv9qtk97y8nvb89rz14kk2k8) not valid after Mon, 27 Feb 2023 03:02:00 UTC, and it is currently Sun, 10 Dec 2023 20:43:13 UTC: x509: certificate has expired or is not yet valid:  RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.2] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1212 13:09:13.984104   51061 out.go:177] ✨  Using the docker driver based on existing profile
I1212 13:09:14.000146   51061 start.go:298] selected driver: docker
I1212 13:09:14.000158   51061 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:false storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1212 13:09:14.000233   51061 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1212 13:09:14.001139   51061 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1212 13:09:14.226155   51061 info.go:266] docker info: {ID:3Y6K:F5AV:56CJ:JDK5:HMTQ:22JX:GZA2:5L73:MPQZ:UNBT:FBKF:JP57 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:false NGoroutines:85 SystemTime:2023-12-12 11:09:14.086449184 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.15.49-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8241238016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr:192.168.65.3 LocalNodeState:error ControlAvailable:false Error:error while loading TLS certificate in /var/lib/docker/swarm/certificates/swarm-node.crt: certificate (1 - sbv9qtk97y8nvb89rz14kk2k8) not valid after Mon, 27 Feb 2023 03:02:00 UTC, and it is currently Sun, 10 Dec 2023 20:43:13 UTC: x509: certificate has expired or is not yet valid:  RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.2] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.0.3] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.13] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1212 13:09:14.234399   51061 cni.go:84] Creating CNI manager for ""
I1212 13:09:14.234815   51061 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:09:14.235309   51061 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:false storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1212 13:09:14.257278   51061 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1212 13:09:14.298337   51061 cache.go:122] Beginning downloading kic base image for docker with docker
I1212 13:09:14.318663   51061 out.go:177] 🚜  Pulling base image ...
I1212 13:09:14.357962   51061 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1212 13:09:14.358023   51061 preload.go:148] Found local preload: /Users/victoria.petrova/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1212 13:09:14.358035   51061 cache.go:57] Caching tarball of preloaded images
I1212 13:09:14.358163   51061 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1212 13:09:14.360246   51061 preload.go:174] Found /Users/victoria.petrova/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1212 13:09:14.360501   51061 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1212 13:09:14.360781   51061 profile.go:148] Saving config to /Users/victoria.petrova/.minikube/profiles/minikube/config.json ...
I1212 13:09:14.443461   51061 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I1212 13:09:14.443476   51061 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I1212 13:09:14.443966   51061 cache.go:195] Successfully downloaded all kic artifacts
I1212 13:09:14.444006   51061 start.go:365] acquiring machines lock for minikube: {Name:mkac7d54810f891967f05fb0fa38aeaf414f32da Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1212 13:09:14.444124   51061 start.go:369] acquired machines lock for "minikube" in 92.806µs
I1212 13:09:14.444148   51061 start.go:96] Skipping create...Using existing machine configuration
I1212 13:09:14.444901   51061 fix.go:54] fixHost starting: 
I1212 13:09:14.445167   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:14.521462   51061 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1212 13:09:14.521494   51061 fix.go:128] unexpected machine state, will restart: <nil>
I1212 13:09:14.560682   51061 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1212 13:09:14.581738   51061 cli_runner.go:164] Run: docker start minikube
I1212 13:09:15.009427   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:15.093548   51061 kic.go:426] container "minikube" state is running.
I1212 13:09:15.095834   51061 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:09:15.182080   51061 profile.go:148] Saving config to /Users/victoria.petrova/.minikube/profiles/minikube/config.json ...
I1212 13:09:15.182982   51061 machine.go:88] provisioning docker machine ...
I1212 13:09:15.184851   51061 ubuntu.go:169] provisioning hostname "minikube"
I1212 13:09:15.186033   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:15.275453   51061 main.go:141] libmachine: Using SSH client type: native
I1212 13:09:15.278860   51061 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x140d900] 0x14109a0 <nil>  [] 0s} 127.0.0.1 49220 <nil> <nil>}
I1212 13:09:15.278886   51061 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1212 13:09:15.568836   51061 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1212 13:09:15.570105   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:15.654439   51061 main.go:141] libmachine: Using SSH client type: native
I1212 13:09:15.654852   51061 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x140d900] 0x14109a0 <nil>  [] 0s} 127.0.0.1 49220 <nil> <nil>}
I1212 13:09:15.654862   51061 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1212 13:09:15.790315   51061 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1212 13:09:15.790354   51061 ubuntu.go:175] set auth options {CertDir:/Users/victoria.petrova/.minikube CaCertPath:/Users/victoria.petrova/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/victoria.petrova/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/victoria.petrova/.minikube/machines/server.pem ServerKeyPath:/Users/victoria.petrova/.minikube/machines/server-key.pem ClientKeyPath:/Users/victoria.petrova/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/victoria.petrova/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/victoria.petrova/.minikube}
I1212 13:09:15.790376   51061 ubuntu.go:177] setting up certificates
I1212 13:09:15.790386   51061 provision.go:83] configureAuth start
I1212 13:09:15.790494   51061 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:09:15.873521   51061 provision.go:138] copyHostCerts
I1212 13:09:15.874690   51061 exec_runner.go:144] found /Users/victoria.petrova/.minikube/ca.pem, removing ...
I1212 13:09:15.874700   51061 exec_runner.go:203] rm: /Users/victoria.petrova/.minikube/ca.pem
I1212 13:09:15.874874   51061 exec_runner.go:151] cp: /Users/victoria.petrova/.minikube/certs/ca.pem --> /Users/victoria.petrova/.minikube/ca.pem (1107 bytes)
I1212 13:09:15.875766   51061 exec_runner.go:144] found /Users/victoria.petrova/.minikube/cert.pem, removing ...
I1212 13:09:15.875772   51061 exec_runner.go:203] rm: /Users/victoria.petrova/.minikube/cert.pem
I1212 13:09:15.875866   51061 exec_runner.go:151] cp: /Users/victoria.petrova/.minikube/certs/cert.pem --> /Users/victoria.petrova/.minikube/cert.pem (1147 bytes)
I1212 13:09:15.876511   51061 exec_runner.go:144] found /Users/victoria.petrova/.minikube/key.pem, removing ...
I1212 13:09:15.876516   51061 exec_runner.go:203] rm: /Users/victoria.petrova/.minikube/key.pem
I1212 13:09:15.876613   51061 exec_runner.go:151] cp: /Users/victoria.petrova/.minikube/certs/key.pem --> /Users/victoria.petrova/.minikube/key.pem (1679 bytes)
I1212 13:09:15.876949   51061 provision.go:112] generating server cert: /Users/victoria.petrova/.minikube/machines/server.pem ca-key=/Users/victoria.petrova/.minikube/certs/ca.pem private-key=/Users/victoria.petrova/.minikube/certs/ca-key.pem org=victoria.petrova.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1212 13:09:16.000843   51061 provision.go:172] copyRemoteCerts
I1212 13:09:16.001338   51061 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1212 13:09:16.001608   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:16.083343   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:16.198184   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1107 bytes)
I1212 13:09:16.230528   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/machines/server.pem --> /etc/docker/server.pem (1229 bytes)
I1212 13:09:16.265623   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1212 13:09:16.300203   51061 provision.go:86] duration metric: configureAuth took 509.797165ms
I1212 13:09:16.300222   51061 ubuntu.go:193] setting minikube options for container-runtime
I1212 13:09:16.300496   51061 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1212 13:09:16.300577   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:16.379850   51061 main.go:141] libmachine: Using SSH client type: native
I1212 13:09:16.380254   51061 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x140d900] 0x14109a0 <nil>  [] 0s} 127.0.0.1 49220 <nil> <nil>}
I1212 13:09:16.380260   51061 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1212 13:09:16.551797   51061 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1212 13:09:16.551816   51061 ubuntu.go:71] root file system type: overlay
I1212 13:09:16.551985   51061 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1212 13:09:16.552130   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:16.698451   51061 main.go:141] libmachine: Using SSH client type: native
I1212 13:09:16.698876   51061 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x140d900] 0x14109a0 <nil>  [] 0s} 127.0.0.1 49220 <nil> <nil>}
I1212 13:09:16.698924   51061 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1212 13:09:16.859812   51061 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1212 13:09:16.860805   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:16.936152   51061 main.go:141] libmachine: Using SSH client type: native
I1212 13:09:16.936546   51061 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x140d900] 0x14109a0 <nil>  [] 0s} 127.0.0.1 49220 <nil> <nil>}
I1212 13:09:16.936559   51061 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1212 13:09:17.095066   51061 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1212 13:09:17.095094   51061 machine.go:91] provisioned docker machine in 1.912112216s
I1212 13:09:17.095112   51061 start.go:300] post-start starting for "minikube" (driver="docker")
I1212 13:09:17.095129   51061 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1212 13:09:17.095323   51061 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1212 13:09:17.095392   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:17.185245   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:17.297270   51061 ssh_runner.go:195] Run: cat /etc/os-release
I1212 13:09:17.306630   51061 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1212 13:09:17.306658   51061 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1212 13:09:17.306673   51061 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1212 13:09:17.306680   51061 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1212 13:09:17.306691   51061 filesync.go:126] Scanning /Users/victoria.petrova/.minikube/addons for local assets ...
I1212 13:09:17.306875   51061 filesync.go:126] Scanning /Users/victoria.petrova/.minikube/files for local assets ...
I1212 13:09:17.306932   51061 start.go:303] post-start completed in 211.815215ms
I1212 13:09:17.307023   51061 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1212 13:09:17.307081   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:17.383057   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:17.488896   51061 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1212 13:09:17.500958   51061 fix.go:56] fixHost completed within 3.056383291s
I1212 13:09:17.500985   51061 start.go:83] releasing machines lock for "minikube", held for 3.056870483s
I1212 13:09:17.501118   51061 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1212 13:09:17.598597   51061 ssh_runner.go:195] Run: cat /version.json
I1212 13:09:17.598653   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:17.599331   51061 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1212 13:09:17.600011   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:17.676791   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:17.676796   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:17.781158   51061 ssh_runner.go:195] Run: systemctl --version
I1212 13:09:18.084137   51061 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1212 13:09:18.098796   51061 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1212 13:09:18.126257   51061 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1212 13:09:18.126380   51061 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1212 13:09:18.138200   51061 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1212 13:09:18.138277   51061 start.go:466] detecting cgroup driver to use...
I1212 13:09:18.138294   51061 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1212 13:09:18.141363   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1212 13:09:18.161942   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1212 13:09:18.175675   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1212 13:09:18.189218   51061 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1212 13:09:18.189322   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1212 13:09:18.202906   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1212 13:09:18.215967   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1212 13:09:18.228934   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1212 13:09:18.242038   51061 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1212 13:09:18.254942   51061 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1212 13:09:18.267986   51061 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1212 13:09:18.279387   51061 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1212 13:09:18.290454   51061 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:09:18.370939   51061 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1212 13:09:18.462278   51061 start.go:466] detecting cgroup driver to use...
I1212 13:09:18.462298   51061 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1212 13:09:18.463791   51061 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1212 13:09:18.479554   51061 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1212 13:09:18.479670   51061 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1212 13:09:18.497430   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1212 13:09:18.534625   51061 ssh_runner.go:195] Run: which cri-dockerd
I1212 13:09:18.562575   51061 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1212 13:09:18.578132   51061 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1212 13:09:18.603765   51061 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1212 13:09:18.702022   51061 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1212 13:09:18.799362   51061 docker.go:535] configuring docker to use "cgroupfs" as cgroup driver...
I1212 13:09:18.799387   51061 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I1212 13:09:18.846196   51061 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:09:18.960084   51061 ssh_runner.go:195] Run: sudo systemctl restart docker
I1212 13:09:19.555809   51061 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1212 13:09:19.639961   51061 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1212 13:09:19.719526   51061 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1212 13:09:19.808251   51061 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:09:19.890825   51061 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1212 13:09:19.913944   51061 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1212 13:09:19.995957   51061 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1212 13:09:20.092737   51061 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1212 13:09:20.093727   51061 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1212 13:09:20.100909   51061 start.go:534] Will wait 60s for crictl version
I1212 13:09:20.101006   51061 ssh_runner.go:195] Run: which crictl
I1212 13:09:20.107279   51061 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1212 13:09:20.173527   51061 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1212 13:09:20.173621   51061 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1212 13:09:20.206271   51061 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1212 13:09:20.261117   51061 out.go:204] 🐳  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1212 13:09:20.262535   51061 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1212 13:09:20.426649   51061 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I1212 13:09:20.426881   51061 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I1212 13:09:20.433616   51061 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 13:09:20.448960   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 13:09:20.533582   51061 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1212 13:09:20.534192   51061 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 13:09:20.578759   51061 docker.go:636] Got preloaded images: -- stdout --
your-kanban-app-image:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
busybox:latest
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1212 13:09:20.579233   51061 docker.go:566] Images already preloaded, skipping extraction
I1212 13:09:20.579704   51061 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1212 13:09:20.610195   51061 docker.go:636] Got preloaded images: -- stdout --
your-kanban-app-image:latest
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
<none>:<none>
busybox:latest
<none>:<none>
<none>:<none>
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1212 13:09:20.610224   51061 cache_images.go:84] Images are preloaded, skipping loading
I1212 13:09:20.610761   51061 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1212 13:09:20.677291   51061 cni.go:84] Creating CNI manager for ""
I1212 13:09:20.677303   51061 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:09:20.678448   51061 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1212 13:09:20.678487   51061 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1212 13:09:20.678638   51061 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1212 13:09:20.679522   51061 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1212 13:09:20.679616   51061 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1212 13:09:20.692379   51061 binaries.go:44] Found k8s binaries, skipping transfer
I1212 13:09:20.692509   51061 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1212 13:09:20.705451   51061 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1212 13:09:20.727950   51061 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1212 13:09:20.748827   51061 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1212 13:09:20.770420   51061 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1212 13:09:20.777704   51061 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1212 13:09:20.794581   51061 certs.go:56] Setting up /Users/victoria.petrova/.minikube/profiles/minikube for IP: 192.168.49.2
I1212 13:09:20.794604   51061 certs.go:190] acquiring lock for shared ca certs: {Name:mkaad487e5386ab5f5004d753b339b8272400346 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:09:20.795288   51061 certs.go:199] skipping minikubeCA CA generation: /Users/victoria.petrova/.minikube/ca.key
I1212 13:09:20.795839   51061 certs.go:199] skipping proxyClientCA CA generation: /Users/victoria.petrova/.minikube/proxy-client-ca.key
I1212 13:09:20.796237   51061 certs.go:315] skipping minikube-user signed cert generation: /Users/victoria.petrova/.minikube/profiles/minikube/client.key
I1212 13:09:20.796728   51061 certs.go:315] skipping minikube signed cert generation: /Users/victoria.petrova/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1212 13:09:20.797284   51061 certs.go:315] skipping aggregator signed cert generation: /Users/victoria.petrova/.minikube/profiles/minikube/proxy-client.key
I1212 13:09:20.797540   51061 certs.go:437] found cert: /Users/victoria.petrova/.minikube/certs/Users/victoria.petrova/.minikube/certs/ca-key.pem (1679 bytes)
I1212 13:09:20.797590   51061 certs.go:437] found cert: /Users/victoria.petrova/.minikube/certs/Users/victoria.petrova/.minikube/certs/ca.pem (1107 bytes)
I1212 13:09:20.797624   51061 certs.go:437] found cert: /Users/victoria.petrova/.minikube/certs/Users/victoria.petrova/.minikube/certs/cert.pem (1147 bytes)
I1212 13:09:20.797654   51061 certs.go:437] found cert: /Users/victoria.petrova/.minikube/certs/Users/victoria.petrova/.minikube/certs/key.pem (1679 bytes)
I1212 13:09:20.801393   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1212 13:09:20.830094   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1212 13:09:20.857737   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1212 13:09:20.887488   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1212 13:09:20.916881   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1212 13:09:20.944832   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1212 13:09:20.975291   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1212 13:09:21.005382   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1212 13:09:21.034084   51061 ssh_runner.go:362] scp /Users/victoria.petrova/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1212 13:09:21.062354   51061 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1212 13:09:21.085515   51061 ssh_runner.go:195] Run: openssl version
I1212 13:09:21.095388   51061 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1212 13:09:21.109777   51061 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1212 13:09:21.116828   51061 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov  7 19:34 /usr/share/ca-certificates/minikubeCA.pem
I1212 13:09:21.116922   51061 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1212 13:09:21.126456   51061 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1212 13:09:21.138859   51061 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1212 13:09:21.145387   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1212 13:09:21.154674   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1212 13:09:21.163836   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1212 13:09:21.173216   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1212 13:09:21.182057   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1212 13:09:21.192312   51061 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1212 13:09:21.202440   51061 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true metrics-server:false storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1212 13:09:21.202600   51061 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 13:09:21.229880   51061 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1212 13:09:21.241642   51061 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1212 13:09:21.242566   51061 kubeadm.go:636] restartCluster start
I1212 13:09:21.242674   51061 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1212 13:09:21.253920   51061 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1212 13:09:21.253999   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 13:09:21.332472   51061 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /Users/victoria.petrova/.kube/config
I1212 13:09:21.332594   51061 kubeconfig.go:146] "minikube" context is missing from /Users/victoria.petrova/.kube/config - will repair!
I1212 13:09:21.332851   51061 lock.go:35] WriteFile acquiring /Users/victoria.petrova/.kube/config: {Name:mkcebaccc5803f5a07ebaa7408a3f37cef4050ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:09:21.350038   51061 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1212 13:09:21.362940   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:21.363044   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:21.376069   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:21.376077   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:21.376184   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:21.390208   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:21.891225   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:21.891581   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:21.920080   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:22.391424   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:22.391746   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:22.424027   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:22.891384   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:22.891871   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:22.921708   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:23.390509   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:23.390883   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:23.421953   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:23.890390   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:23.891916   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:23.922169   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:24.391343   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:24.391473   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:24.408809   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:24.891209   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:24.891583   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:24.923759   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:25.391509   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:25.391868   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:25.422609   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:25.890851   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:25.890990   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:25.909238   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:26.391552   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:26.391949   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:26.422694   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:26.890470   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:26.890818   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:26.920887   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:27.391442   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:27.391803   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:27.421466   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:27.890507   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:27.890912   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:27.918718   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:28.391115   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:28.391475   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:28.419707   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:28.891465   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:28.891878   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:28.922013   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:29.391288   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:29.391448   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:29.411234   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:29.890365   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:29.890586   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:29.916918   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:30.391401   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:30.391877   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:30.421862   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:30.890969   51061 api_server.go:166] Checking apiserver status ...
I1212 13:09:30.891298   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1212 13:09:30.921816   51061 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1212 13:09:31.363406   51061 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1212 13:09:31.363471   51061 kubeadm.go:1128] stopping kube-system containers ...
I1212 13:09:31.363730   51061 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1212 13:09:31.414157   51061 docker.go:462] Stopping containers: [e240c6594ba2 a8935e465df5 f2bbfa5482d1 47728fe0b61d f7a4825208b4 ee71ad639290 2a406cad2d3a fcc31f454821 f55a3a22090b 053eff47be48 86cae000bb98 852b6f1ca29f 01423753b518 7b507e64e612 cc71afe9c9fa cb4aa8fc0fd1 da3937df4962 72992afee7b6 abbfa626917f d9190f347114 c9a9100a3525 d683ee5c80a4 ae912a2719df 3f63ccbb60a7 258d260be468]
I1212 13:09:31.414249   51061 ssh_runner.go:195] Run: docker stop e240c6594ba2 a8935e465df5 f2bbfa5482d1 47728fe0b61d f7a4825208b4 ee71ad639290 2a406cad2d3a fcc31f454821 f55a3a22090b 053eff47be48 86cae000bb98 852b6f1ca29f 01423753b518 7b507e64e612 cc71afe9c9fa cb4aa8fc0fd1 da3937df4962 72992afee7b6 abbfa626917f d9190f347114 c9a9100a3525 d683ee5c80a4 ae912a2719df 3f63ccbb60a7 258d260be468
I1212 13:09:31.440920   51061 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1212 13:09:31.456625   51061 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1212 13:09:31.470273   51061 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Dec 12 11:01 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Dec 12 11:01 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Dec 12 11:01 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Dec 12 11:01 /etc/kubernetes/scheduler.conf

I1212 13:09:31.470373   51061 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1212 13:09:31.484840   51061 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1212 13:09:31.497494   51061 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1212 13:09:31.509027   51061 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1212 13:09:31.509172   51061 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1212 13:09:31.520892   51061 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1212 13:09:31.532653   51061 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1212 13:09:31.532766   51061 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1212 13:09:31.544723   51061 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1212 13:09:31.557124   51061 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1212 13:09:31.557136   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:31.624121   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:32.327852   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:32.490286   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:32.561699   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:32.676994   51061 api_server.go:52] waiting for apiserver process to appear ...
I1212 13:09:32.677207   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:32.697255   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:33.270616   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:33.771105   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:34.271225   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:34.346763   51061 api_server.go:72] duration metric: took 1.669792518s to wait for apiserver process to appear ...
I1212 13:09:34.346792   51061 api_server.go:88] waiting for apiserver healthz status ...
I1212 13:09:34.346819   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:34.349322   51061 api_server.go:269] stopped: https://127.0.0.1:49219/healthz: Get "https://127.0.0.1:49219/healthz": EOF
I1212 13:09:34.349344   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:34.352021   51061 api_server.go:269] stopped: https://127.0.0.1:49219/healthz: Get "https://127.0.0.1:49219/healthz": EOF
I1212 13:09:34.852978   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:37.655447   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1212 13:09:37.655464   51061 api_server.go:103] status: https://127.0.0.1:49219/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1212 13:09:37.655476   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:37.675282   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1212 13:09:37.675297   51061 api_server.go:103] status: https://127.0.0.1:49219/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1212 13:09:37.852191   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:37.870143   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1212 13:09:37.870170   51061 api_server.go:103] status: https://127.0.0.1:49219/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1212 13:09:38.353397   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:38.373082   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1212 13:09:38.373108   51061 api_server.go:103] status: https://127.0.0.1:49219/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1212 13:09:38.852326   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:38.870787   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1212 13:09:38.870812   51061 api_server.go:103] status: https://127.0.0.1:49219/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1212 13:09:39.352216   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:39.361625   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 200:
ok
I1212 13:09:39.381259   51061 api_server.go:141] control plane version: v1.27.4
I1212 13:09:39.381276   51061 api_server.go:131] duration metric: took 5.034506055s to wait for apiserver health ...
I1212 13:09:39.381283   51061 cni.go:84] Creating CNI manager for ""
I1212 13:09:39.381297   51061 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1212 13:09:39.402063   51061 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1212 13:09:39.438785   51061 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1212 13:09:39.459424   51061 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1212 13:09:39.498323   51061 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 13:09:39.523481   51061 system_pods.go:59] 7 kube-system pods found
I1212 13:09:39.523502   51061 system_pods.go:61] "coredns-5d78c9869d-4qlxg" [7e199af7-5e38-4635-ba40-d824aab371f9] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1212 13:09:39.523507   51061 system_pods.go:61] "etcd-minikube" [6f41976c-2a86-4b20-9616-25ac569f7386] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1212 13:09:39.523513   51061 system_pods.go:61] "kube-apiserver-minikube" [f448e0b0-c505-4477-ba39-b977ef8935b5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1212 13:09:39.523519   51061 system_pods.go:61] "kube-controller-manager-minikube" [3ffe3db8-93c2-416b-b0af-a589b71057c4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1212 13:09:39.523523   51061 system_pods.go:61] "kube-proxy-gzqzq" [fa244e6e-6ba4-4989-ab7e-7e7d3318ca23] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1212 13:09:39.523530   51061 system_pods.go:61] "kube-scheduler-minikube" [7c03c29c-502f-4e57-861b-b86768c08c8e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1212 13:09:39.523541   51061 system_pods.go:61] "storage-provisioner" [1125b31a-0ff3-4f5f-b242-15669a3d68db] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1212 13:09:39.523545   51061 system_pods.go:74] duration metric: took 25.210768ms to wait for pod list to return data ...
I1212 13:09:39.523554   51061 node_conditions.go:102] verifying NodePressure condition ...
I1212 13:09:39.551571   51061 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1212 13:09:39.552072   51061 node_conditions.go:123] node cpu capacity is 4
I1212 13:09:39.552510   51061 node_conditions.go:105] duration metric: took 28.946826ms to run NodePressure ...
I1212 13:09:39.552560   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1212 13:09:40.378319   51061 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1212 13:09:40.393374   51061 ops.go:34] apiserver oom_adj: -16
I1212 13:09:40.393385   51061 kubeadm.go:640] restartCluster took 19.150915774s
I1212 13:09:40.393402   51061 kubeadm.go:406] StartCluster complete in 19.191085697s
I1212 13:09:40.394062   51061 settings.go:142] acquiring lock: {Name:mkc99e4833fa2981dc0c10107025acee3413a27c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:09:40.394239   51061 settings.go:150] Updating kubeconfig:  /Users/victoria.petrova/.kube/config
I1212 13:09:40.394902   51061 lock.go:35] WriteFile acquiring /Users/victoria.petrova/.kube/config: {Name:mkcebaccc5803f5a07ebaa7408a3f37cef4050ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1212 13:09:40.396037   51061 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1212 13:09:40.396424   51061 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1212 13:09:40.396395   51061 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1212 13:09:40.397306   51061 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1212 13:09:40.397365   51061 addons.go:69] Setting dashboard=true in profile "minikube"
I1212 13:09:40.397371   51061 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1212 13:09:40.397713   51061 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1212 13:09:40.397723   51061 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1212 13:09:40.397727   51061 addons.go:231] Setting addon dashboard=true in "minikube"
W1212 13:09:40.397735   51061 addons.go:240] addon dashboard should already be in state true
W1212 13:09:40.397753   51061 addons.go:240] addon storage-provisioner should already be in state true
I1212 13:09:40.398577   51061 host.go:66] Checking if "minikube" exists ...
I1212 13:09:40.398577   51061 host.go:66] Checking if "minikube" exists ...
I1212 13:09:40.398782   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:40.400138   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:40.400215   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:40.450571   51061 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1212 13:09:40.451415   51061 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1212 13:09:40.472374   51061 out.go:177] 🔎  Verifying Kubernetes components...
I1212 13:09:40.528952   51061 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1212 13:09:40.573914   51061 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1212 13:09:40.573823   51061 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1212 13:09:40.594158   51061 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W1212 13:09:40.594206   51061 addons.go:240] addon default-storageclass should already be in state true
I1212 13:09:40.665638   51061 host.go:66] Checking if "minikube" exists ...
I1212 13:09:40.665829   51061 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1212 13:09:40.665842   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1212 13:09:40.665976   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:40.667559   51061 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1212 13:09:40.686635   51061 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1212 13:09:40.706667   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1212 13:09:40.706694   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1212 13:09:40.706850   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:40.770056   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:40.791149   51061 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1212 13:09:40.791157   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1212 13:09:40.791255   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1212 13:09:40.813480   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:40.866052   51061 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1212 13:09:40.866796   51061 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1212 13:09:40.885379   51061 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49220 SSHKeyPath:/Users/victoria.petrova/.minikube/machines/minikube/id_rsa Username:docker}
I1212 13:09:40.954892   51061 api_server.go:52] waiting for apiserver process to appear ...
I1212 13:09:40.955014   51061 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1212 13:09:41.053134   51061 api_server.go:72] duration metric: took 601.663133ms to wait for apiserver process to appear ...
I1212 13:09:41.053146   51061 api_server.go:88] waiting for apiserver healthz status ...
I1212 13:09:41.053159   51061 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49219/healthz ...
I1212 13:09:41.065308   51061 api_server.go:279] https://127.0.0.1:49219/healthz returned 200:
ok
I1212 13:09:41.067692   51061 api_server.go:141] control plane version: v1.27.4
I1212 13:09:41.067709   51061 api_server.go:131] duration metric: took 14.557563ms to wait for apiserver health ...
I1212 13:09:41.067716   51061 system_pods.go:43] waiting for kube-system pods to appear ...
I1212 13:09:41.077916   51061 system_pods.go:59] 7 kube-system pods found
I1212 13:09:41.077952   51061 system_pods.go:61] "coredns-5d78c9869d-4qlxg" [7e199af7-5e38-4635-ba40-d824aab371f9] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1212 13:09:41.077969   51061 system_pods.go:61] "etcd-minikube" [6f41976c-2a86-4b20-9616-25ac569f7386] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1212 13:09:41.077981   51061 system_pods.go:61] "kube-apiserver-minikube" [f448e0b0-c505-4477-ba39-b977ef8935b5] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1212 13:09:41.077991   51061 system_pods.go:61] "kube-controller-manager-minikube" [3ffe3db8-93c2-416b-b0af-a589b71057c4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1212 13:09:41.078001   51061 system_pods.go:61] "kube-proxy-gzqzq" [fa244e6e-6ba4-4989-ab7e-7e7d3318ca23] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1212 13:09:41.078010   51061 system_pods.go:61] "kube-scheduler-minikube" [7c03c29c-502f-4e57-861b-b86768c08c8e] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1212 13:09:41.078029   51061 system_pods.go:61] "storage-provisioner" [1125b31a-0ff3-4f5f-b242-15669a3d68db] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1212 13:09:41.078035   51061 system_pods.go:74] duration metric: took 10.315121ms to wait for pod list to return data ...
I1212 13:09:41.078045   51061 kubeadm.go:581] duration metric: took 626.577995ms to wait for : map[apiserver:true system_pods:true] ...
I1212 13:09:41.078056   51061 node_conditions.go:102] verifying NodePressure condition ...
I1212 13:09:41.082883   51061 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1212 13:09:41.082893   51061 node_conditions.go:123] node cpu capacity is 4
I1212 13:09:41.082899   51061 node_conditions.go:105] duration metric: took 4.839686ms to run NodePressure ...
I1212 13:09:41.082907   51061 start.go:228] waiting for startup goroutines ...
I1212 13:09:41.102295   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1212 13:09:41.102307   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1212 13:09:41.168583   51061 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1212 13:09:41.190024   51061 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1212 13:09:41.296658   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1212 13:09:41.296686   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1212 13:09:41.565066   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1212 13:09:41.565132   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1212 13:09:41.679255   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1212 13:09:41.679267   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1212 13:09:41.791609   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1212 13:09:41.791638   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1212 13:09:42.082322   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1212 13:09:42.082336   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1212 13:09:42.187779   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1212 13:09:42.187791   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1212 13:09:42.279273   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1212 13:09:42.279284   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1212 13:09:42.301956   51061 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1212 13:09:42.301969   51061 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1212 13:09:42.363705   51061 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1212 13:09:42.792384   51061 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.623784592s)
I1212 13:09:43.553610   51061 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.363554079s)
I1212 13:09:44.189982   51061 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.826257279s)
I1212 13:09:44.234200   51061 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1212 13:09:44.307537   51061 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner, dashboard
I1212 13:09:44.348000   51061 addons.go:502] enable addons completed in 3.951904317s: enabled=[default-storageclass storage-provisioner dashboard]
I1212 13:09:44.348026   51061 start.go:233] waiting for cluster config update ...
I1212 13:09:44.348045   51061 start.go:242] writing updated cluster config ...
I1212 13:09:44.349358   51061 ssh_runner.go:195] Run: rm -f paused
I1212 13:09:44.429503   51061 start.go:600] kubectl: 1.25.2, cluster: 1.27.4 (minor skew: 2)
I1212 13:09:44.449450   51061 out.go:177] 
W1212 13:09:44.469125   51061 out.go:239] ❗  /usr/local/bin/kubectl is version 1.25.2, which may have incompatibilities with Kubernetes 1.27.4.
I1212 13:09:44.523825   51061 out.go:177]     ▪ Want kubectl v1.27.4? Try 'minikube kubectl -- get pods -A'
I1212 13:09:44.542584   51061 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Start docker client with request timeout 0s"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Loaded network plugin cni"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Docker Info: &{ID:92f16f8c-4cb0-40bc-b0c3-3b1b0a94a025 Containers:43 ContainersRunning:0 ContainersPaused:0 ContainersStopped:43 Images:23 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:35 SystemTime:2023-12-12T11:09:20.081188704Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:5.15.49-linuxkit OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000224070 NCPU:4 MemTotal:8241238016 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 12 11:09:20 minikube cri-dockerd[1077]: time="2023-12-12T11:09:20Z" level=info msg="Start cri-dockerd grpc backend"
Dec 12 11:09:20 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-deployment-fd66fcbf4-w4pwj_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"991d718732e2df5535c24e59abf5b0d918ae7bc5ff7f4194cd0b4dd0701f72c0\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-deployment-fd66fcbf4-w4pwj_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ff90a9304d33b9d6e447157125feea9496f19c7feef7f90019f0fe00db394c63\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-deployment-fd66fcbf4-2m6p9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9bbc7acf9756990ab38d6c73fd9a590711e35f6ac2e07d2b471cdec41dc011eb\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"flask-deployment-fd66fcbf4-2m6p9_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5a6905b81fc7b7d58def427ae98ef8e3235f9b708accd81ef85fe125d988521e\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-4qlxg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"47728fe0b61ddd82fef27578f7e17157fac73ecc2d721b850fdce639bfc15b33\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-4qlxg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"fcc31f454821c0f65839c64331836e4df1a20ca62b01db20d3301690fee662f2\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-c745v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"21063f0649ea249ba12603187f573729b5c419a8f55ce9d3c273582d0ff58573\""
Dec 12 11:09:32 minikube cri-dockerd[1077]: time="2023-12-12T11:09:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-c745v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"93b5b0bbc52a3f42e7176999375b2cd4657394a28f1671a2a5098b5df9cf7d0c\""
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"load-generator_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a9bc5b57aee183a56968496f6212e2585b37cb79bc993725d7ce7c69450dbc94\""
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"load-generator_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3eacb554aac71363572da7b7becc602c70373f00ee30469c1b0bf0ed11a21771\""
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-gfshs_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8ae0ff0d2c0e67a7ff754c5b44909d2f23816b6fadcc0197ba6dd30498b48528\""
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"ae912a2719dfe0fdf6895d1546d179c14e1c88624c0dd70ffd719f9470ab839d\". Proceed without further sandbox information."
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"d683ee5c80a4ce0012cf0b592ce236387cfe93d75d1bcc7fc66d7e007a933379\". Proceed without further sandbox information."
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"258d260be4687eabccdfa5a38b3874ccd31f4368e5233b10d03515cc363bafb9\". Proceed without further sandbox information."
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fb1bf9c3b2e5f584f41f33fa2e6e17a584b59bbf648d8658dffaf75350bfa1cb/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fc57ecc56a0c1a604d221341bc21c6c3bebb2bb75e7e2a01f685e4d383a36db6/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e91cf2741051658083b74791b9035f691659b5e157ed7d53eefb5e7267262f19/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:33 minikube cri-dockerd[1077]: time="2023-12-12T11:09:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0be9e3255301851b5753a11be87372225adf0b659f228458e2d5373bf1db4967/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:34 minikube cri-dockerd[1077]: time="2023-12-12T11:09:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"load-generator_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a9bc5b57aee183a56968496f6212e2585b37cb79bc993725d7ce7c69450dbc94\""
Dec 12 11:09:34 minikube cri-dockerd[1077]: time="2023-12-12T11:09:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5dd9cbfd69-c745v_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"21063f0649ea249ba12603187f573729b5c419a8f55ce9d3c273582d0ff58573\""
Dec 12 11:09:34 minikube cri-dockerd[1077]: time="2023-12-12T11:09:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5d78c9869d-4qlxg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"47728fe0b61ddd82fef27578f7e17157fac73ecc2d721b850fdce639bfc15b33\""
Dec 12 11:09:34 minikube cri-dockerd[1077]: time="2023-12-12T11:09:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-5c5cfc8747-gfshs_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8ae0ff0d2c0e67a7ff754c5b44909d2f23816b6fadcc0197ba6dd30498b48528\""
Dec 12 11:09:37 minikube cri-dockerd[1077]: time="2023-12-12T11:09:37Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 12 11:09:39 minikube cri-dockerd[1077]: time="2023-12-12T11:09:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/47a820c9b3acf34979197a4d5900ea39536188f9039b910485a77740b43723c5/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:39 minikube cri-dockerd[1077]: time="2023-12-12T11:09:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/36689318c85f710e8fd405c1d8274c79522d0f64902191a037fc461ebc791bd6/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:40 minikube cri-dockerd[1077]: time="2023-12-12T11:09:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b867234a6c0fbac11749091cc94b4891ec6e9bb69bf9a2db9dbd580273f3c903/resolv.conf as [nameserver 192.168.65.2 options ndots:0]"
Dec 12 11:09:40 minikube cri-dockerd[1077]: time="2023-12-12T11:09:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa2ef4eb4f782b75f851673978837fb6fc36d618c16475295ae9918def74f7ff/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 11:09:40 minikube cri-dockerd[1077]: time="2023-12-12T11:09:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b7b0da6bedcb0ef5e94e1f53b1c738666d2120b1c0accd8e5853e4610ead1310/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 11:09:40 minikube cri-dockerd[1077]: time="2023-12-12T11:09:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5602e72f3699d6def81323d3553c4cef9a0d36e2c7113b036ab48d64f9e6a596/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 11:09:41 minikube cri-dockerd[1077]: time="2023-12-12T11:09:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d6b9a44625992353a56ce5ca7f6237fc03a462a0d595fc88c456bf4c141adbee/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 11:09:41 minikube cri-dockerd[1077]: time="2023-12-12T11:09:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/772a63a12257bc3056691513d4da4054bbf581c8d8fc0f40b9640148f9da4386/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 12 11:09:43 minikube cri-dockerd[1077]: time="2023-12-12T11:09:43Z" level=info msg="Stop pulling image busybox:latest: Status: Image is up to date for busybox:latest"
Dec 12 11:09:44 minikube dockerd[826]: time="2023-12-12T11:09:44.979711629Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:09:44 minikube dockerd[826]: time="2023-12-12T11:09:44.979846790Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:09:46 minikube dockerd[826]: time="2023-12-12T11:09:46.761476371Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:09:46 minikube dockerd[826]: time="2023-12-12T11:09:46.761626490Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:09:47 minikube dockerd[826]: time="2023-12-12T11:09:47.696813223Z" level=info msg="ignoring event" container=df4e9a8aad8f38f7597f69f9bc80be19e13b49c6327d4f28683d73393eaf8719 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 12 11:10:00 minikube dockerd[826]: time="2023-12-12T11:10:00.543540146Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:10:00 minikube dockerd[826]: time="2023-12-12T11:10:00.543715259Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:10:02 minikube dockerd[826]: time="2023-12-12T11:10:02.327281673Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:10:02 minikube dockerd[826]: time="2023-12-12T11:10:02.327483506Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:10:08 minikube dockerd[826]: time="2023-12-12T11:10:08.763561582Z" level=info msg="ignoring event" container=44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 12 11:10:10 minikube dockerd[826]: time="2023-12-12T11:10:10.328600260Z" level=info msg="ignoring event" container=b951b1fd7b339e19ea0210db9a822e6a95b9f433a384268e804787302d5f99f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 12 11:10:25 minikube dockerd[826]: time="2023-12-12T11:10:25.535052615Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:10:25 minikube dockerd[826]: time="2023-12-12T11:10:25.535132070Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:10:28 minikube dockerd[826]: time="2023-12-12T11:10:28.556296673Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 12 11:10:28 minikube dockerd[826]: time="2023-12-12T11:10:28.556814994Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 12 11:10:40 minikube dockerd[826]: time="2023-12-12T11:10:40.211233194Z" level=warning msg="no trace recorder found, skipping"
Dec 12 11:10:45 minikube dockerd[826]: time="2023-12-12T11:10:45.063943787Z" level=info msg="ignoring event" container=1b3c411269a62d76d03d8a766bb75501334dd17c87781376d960d3cb697a0167 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                             CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
1b3c411269a62       f466468864b7a                                                                     26 seconds ago       Exited              kube-controller-manager     21                  e91cf27410516       kube-controller-manager-minikube
4c33ac650a2e9       6e38f40d628db                                                                     38 seconds ago       Running             storage-provisioner         18                  36689318c85f7       storage-provisioner
3d48038af1ed4       busybox@sha256:1ceb872bcc68a8fcd34c97952658b58086affdcb604c90c1dee2735bde5edc2f   About a minute ago   Running             load-generator              8                   aa2ef4eb4f782       load-generator
70c650673c2ac       115053965e86b                                                                     About a minute ago   Running             dashboard-metrics-scraper   10                  5602e72f3699d       dashboard-metrics-scraper-5dd9cbfd69-c745v
3e1a4b1ade84c       07655ddf2eebe                                                                     About a minute ago   Running             kubernetes-dashboard        13                  b7b0da6bedcb0       kubernetes-dashboard-5c5cfc8747-gfshs
283645b813ece       ead0a4a53df89                                                                     About a minute ago   Running             coredns                     10                  b867234a6c0fb       coredns-5d78c9869d-4qlxg
b951b1fd7b339       6e38f40d628db                                                                     About a minute ago   Exited              storage-provisioner         17                  36689318c85f7       storage-provisioner
778da19335dec       6848d7eda0341                                                                     About a minute ago   Running             kube-proxy                  6                   47a820c9b3acf       kube-proxy-gzqzq
9cd0629018a86       e7972205b6614                                                                     About a minute ago   Running             kube-apiserver              3                   0be9e32553018       kube-apiserver-minikube
0e5a9c02c1440       86b6af7dd652c                                                                     About a minute ago   Running             etcd                        3                   fc57ecc56a0c1       etcd-minikube
c8c2f8bcea78d       98ef2570f3cde                                                                     About a minute ago   Running             kube-scheduler              6                   fb1bf9c3b2e5f       kube-scheduler-minikube
d985d4f29c4a7       busybox@sha256:1ceb872bcc68a8fcd34c97952658b58086affdcb604c90c1dee2735bde5edc2f   8 minutes ago        Exited              load-generator              7                   a9bc5b57aee18       load-generator
b193cb8eb2a6e       07655ddf2eebe                                                                     9 minutes ago        Exited              kubernetes-dashboard        12                  8ae0ff0d2c0e6       kubernetes-dashboard-5c5cfc8747-gfshs
f2bbfa5482d17       ead0a4a53df89                                                                     9 minutes ago        Exited              coredns                     9                   47728fe0b61dd       coredns-5d78c9869d-4qlxg
b1b78f3b7f934       115053965e86b                                                                     9 minutes ago        Exited              dashboard-metrics-scraper   9                   21063f0649ea2       dashboard-metrics-scraper-5dd9cbfd69-c745v
ee71ad639290a       6848d7eda0341                                                                     9 minutes ago        Exited              kube-proxy                  5                   f55a3a22090b0       kube-proxy-gzqzq
86cae000bb981       98ef2570f3cde                                                                     9 minutes ago        Exited              kube-scheduler              5                   852b6f1ca29f4       kube-scheduler-minikube
01423753b5184       86b6af7dd652c                                                                     9 minutes ago        Exited              etcd                        2                   7b507e64e6127       etcd-minikube
cc71afe9c9fad       e7972205b6614                                                                     9 minutes ago        Exited              kube-apiserver              2                   da3937df4962c       kube-apiserver-minikube
d93600190b808       912461e5b1d0e                                                                     38 hours ago         Exited              flask-app-container         0                   ff90a9304d33b       flask-deployment-fd66fcbf4-w4pwj
a936c51fbb89b       912461e5b1d0e                                                                     38 hours ago         Exited              flask-app-container         0                   5a6905b81fc7b       flask-deployment-fd66fcbf4-2m6p9

* 
* ==> coredns [283645b813ec] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:58408 - 11488 "HINFO IN 17694536266895787.4878446060470842045. udp 55 false 512" NXDOMAIN qr,rd,ra 55 0.021094546s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [f2bbfa5482d1] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 8846d9ca81164c00fa03e78dfcf1a6846552cc49335bc010218794b8cfaf537759aa4b596e7dc20c0f618e8eb07603c0139662b99dfa3de45b176fbe7fb57ce1
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:37038 - 10713 "HINFO IN 7665376796029778693.1826129463705032266. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.034870129s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_07T21_34_12_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 07 Nov 2023 19:34:08 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 12 Dec 2023 11:10:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 12 Dec 2023 11:09:37 +0000   Tue, 12 Dec 2023 09:25:14 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 12 Dec 2023 11:09:37 +0000   Tue, 12 Dec 2023 09:25:14 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 12 Dec 2023 11:09:37 +0000   Tue, 12 Dec 2023 09:25:14 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 12 Dec 2023 11:09:37 +0000   Tue, 12 Dec 2023 09:25:14 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             8048084Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  61202244Ki
  hugepages-2Mi:      0
  memory:             8048084Ki
  pods:               110
System Info:
  Machine ID:                 6ef734ded5eb4fdba4104159b4bf2f9f
  System UUID:                6ef734ded5eb4fdba4104159b4bf2f9f
  Boot ID:                    34e4d0d5-1b09-4f38-b55f-2c762eeefc87
  Kernel Version:             5.15.49-linuxkit
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     flask-deployment-fd66fcbf4-2m6p9              50m (1%!)(MISSING)      50m (1%!)(MISSING)    128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     37h
  default                     flask-deployment-fd66fcbf4-w4pwj              50m (1%!)(MISSING)      50m (1%!)(MISSING)    128Mi (1%!)(MISSING)       128Mi (1%!)(MISSING)     37h
  default                     load-generator                                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         37h
  kube-system                 coredns-5d78c9869d-4qlxg                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     34d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         57m
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         57m
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kube-system                 kube-proxy-gzqzq                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kubernetes-dashboard        dashboard-metrics-scraper-5dd9cbfd69-c745v    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  kubernetes-dashboard        kubernetes-dashboard-5c5cfc8747-gfshs         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  100m (2%!)(MISSING)
  memory             426Mi (5%!)(MISSING)  426Mi (5%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 53m                    kube-proxy       
  Normal  Starting                 9m28s                  kube-proxy       
  Normal  Starting                 79s                    kube-proxy       
  Normal  Starting                 57m                    kube-proxy       
  Normal  Starting                 69m                    kube-proxy       
  Normal  NodeNotReady             105m (x9 over 36h)     node-controller  Node minikube status is now: NodeNotReady
  Normal  NodeHasSufficientMemory  105m (x31 over 38h)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    105m (x31 over 38h)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     105m (x31 over 38h)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeReady                105m (x13 over 36h)    kubelet          Node minikube status is now: NodeReady
  Normal  NodeAllocatableEnforced  69m                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     69m (x7 over 69m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 69m                    kubelet          Starting kubelet.
  Normal  NodeHasNoDiskPressure    69m (x8 over 69m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  69m (x8 over 69m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           69m                    node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 57m                    kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  57m (x8 over 57m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    57m (x8 over 57m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     57m (x7 over 57m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  57m                    kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 53m                    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  53m                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     53m (x7 over 53m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  53m (x8 over 53m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    53m (x8 over 53m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  Starting                 9m35s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  9m34s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientPID     9m34s (x7 over 9m34s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    9m34s (x8 over 9m34s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  9m34s (x8 over 9m34s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  Starting                 87s                    kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  87s                    kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  86s (x8 over 87s)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    86s (x8 over 87s)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     86s (x7 over 87s)      kubelet          Node minikube status is now: NodeHasSufficientPID

* 
* ==> dmesg <==
* [  +0.022267] rcu: Stack dump where RCU GP kthread last ran:
[  +0.005452] NMI backtrace for cpu 1
[  +0.000006] CPU: 1 PID: 0 Comm: swapper/1 Tainted: G           O    T 5.15.49-linuxkit #1
[  +0.000004] RIP: 0010:native_apic_mem_write+0x8/0x9
[  +0.693261] Code: 48 c7 c0 f0 bd 01 00 48 8b 14 fd 80 7a 30 ab 8b 3c 02 0f ae f0 0f ae e8 ba 00 08 00 00 e9 5c fa ff ff 89 ff 89 b7 00 d0 5f ff <c3> 89 ff 8b 87 00 d0 5f ff c3 0f 1f 44 00 00 b8 01 00 00 00 c3 0f
[  +0.000002] RSP: 0018:ffffaf4a401e0fd8 EFLAGS: 00010046
[  +0.000003] RAX: ffffffffaa04c785 RBX: 0000000000000000 RCX: 0000000000000000
[  +0.000001] RDX: 00004e23843ae827 RSI: 0000000000000000 RDI: 00000000000000b0
[  +0.000001] RBP: ffffaf4a4019be48 R08: 0000000006e6df5a R09: 0000000000000000
[  +0.000002] R10: 0000000000000000 R11: ffffaf4a401e0ff8 R12: 0000000000000001
[  +0.000001] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001] FS:  0000000000000000(0000) GS:ffff925ff1880000(0000) knlGS:0000000000000000
[  +0.000002] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 000000c0001ab008 CR3: 000000011e7e2004 CR4: 0000000000770ea0
[  +0.000002] PKRU: 55555554
[  +0.000001] Call Trace:
[  +0.000002]  <IRQ>
[  +0.000001]  __sysvec_apic_timer_interrupt+0x1c/0xdb
[  +0.000008]  sysvec_apic_timer_interrupt+0x61/0x7d
[  +0.000005]  </IRQ>
[  +0.000000]  <TASK>
[  +0.000001]  asm_sysvec_apic_timer_interrupt+0x12/0x20
[  +0.000004] RIP: 0010:native_safe_halt+0x7/0x8
[  +0.000002] Code: 60 02 df f0 83 44 24 fc 00 48 8b 00 a8 08 74 0b 65 81 25 3c 40 55 55 ff ff ff 7f c3 e8 f1 a0 58 ff f4 c3 e8 ea a0 58 ff fb f4 <c3> 0f 1f 44 00 00 53 31 ff 65 8b 35 1d d6 54 55 e8 72 45 68 ff e8
[  +0.000002] RSP: 0018:ffffaf4a4019bef0 EFLAGS: 00000202
[  +0.000001] RAX: ffffffffaaac7a10 RBX: ffff925ec03de440 RCX: 0000000000000000
[  +0.000002] RDX: ffff925ff18acc20 RSI: 0000000000000001 RDI: 0000000002a3674c
[  +0.000001] RBP: 0000000000000000 R08: 0000000000000001 R09: 000000000000001f
[  +0.000001] R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
[  +0.000001] R13: 0000000000000001 R14: 0000000000000000 R15: 0000000000000000
[  +0.000001]  ? __sched_text_end+0x1/0x1
[  +0.000006]  arch_safe_halt+0x5/0x8
[  +0.000003]  default_idle_call+0x2e/0x4c
[  +0.000002]  do_idle+0xde/0x1f7
[  +0.000003]  cpu_startup_entry+0x1d/0x1f
[  +0.000002]  secondary_startup_64_no_verify+0xb0/0xbb
[  +0.000004]  </TASK>
[Dec11 20:54] Hangcheck: hangcheck value past margin!
[Dec11 23:17] Hangcheck: hangcheck value past margin!
[Dec11 23:40] Hangcheck: hangcheck value past margin!
[Dec11 23:57] Hangcheck: hangcheck value past margin!
[Dec12 00:25] systemd-journald[388620]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 02:46] Hangcheck: hangcheck value past margin!
[Dec12 04:47] Hangcheck: hangcheck value past margin!
[Dec12 06:48] Hangcheck: hangcheck value past margin!
[Dec12 06:52] Hangcheck: hangcheck value past margin!
[Dec12 07:07] Hangcheck: hangcheck value past margin!
[ +40.467719] systemd-journald[526014]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 07:39] Hangcheck: hangcheck value past margin!
[  +3.304979] systemd-journald[528255]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 07:56] Hangcheck: hangcheck value past margin!
[  +1.120322] systemd-journald[528704]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 08:30] Hangcheck: hangcheck value past margin!
[  +1.086255] systemd-journald[533047]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 08:46] Hangcheck: hangcheck value past margin!
[  +1.580284] systemd-journald[534539]: File /run/log/journal/fd56a87ad73e4b0090b56d7d650f9d08/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Dec12 09:08] Hangcheck: hangcheck value past margin!
[Dec12 09:18] Hangcheck: hangcheck value past margin!
[Dec12 09:24] Hangcheck: hangcheck value past margin!
[Dec12 10:59] Hangcheck: hangcheck value past margin!

* 
* ==> etcd [01423753b518] <==
* {"level":"info","ts":"2023-12-12T11:01:26.353Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-12-12T11:01:26.357Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-12-12T11:01:26.357Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-12-12T11:01:26.357Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-12-12T11:01:26.358Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-12-12T11:01:26.358Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-12-12T11:01:26.366Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"7.309819ms"}
{"level":"info","ts":"2023-12-12T11:01:26.864Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":30003,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-12-12T11:01:26.864Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":4038656,"backend-size":"4.0 MB","backend-size-in-use-bytes":3878912,"backend-size-in-use":"3.9 MB"}
{"level":"info","ts":"2023-12-12T11:01:27.204Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":38707}
{"level":"info","ts":"2023-12-12T11:01:27.204Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-12-12T11:01:27.205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 6"}
{"level":"info","ts":"2023-12-12T11:01:27.205Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 6, commit: 38707, applied: 30003, lastindex: 38707, lastterm: 6]"}
{"level":"info","ts":"2023-12-12T11:01:27.205Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-12T11:01:27.205Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-12-12T11:01:27.205Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-12-12T11:01:27.208Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-12-12T11:01:27.209Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":30002}
{"level":"info","ts":"2023-12-12T11:01:27.260Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":31167}
{"level":"info","ts":"2023-12-12T11:01:27.264Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-12-12T11:01:27.269Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-12-12T11:01:27.272Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-12-12T11:01:27.272Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-12T11:01:27.273Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-12-12T11:01:27.273Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:01:27.273Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:01:27.273Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:01:27.282Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-12-12T11:01:27.282Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:01:27.282Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:01:27.282Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-12-12T11:01:27.282Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 6"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 6"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 7"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 7"}
{"level":"info","ts":"2023-12-12T11:01:28.006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 7"}
{"level":"info","ts":"2023-12-12T11:01:28.008Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-12-12T11:01:28.008Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-12T11:01:28.008Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-12T11:01:28.010Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-12-12T11:01:28.010Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-12-12T11:01:28.008Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-12-12T11:01:28.010Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-12-12T11:08:35.388Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-12-12T11:08:35.388Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-12-12T11:08:35.462Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-12-12T11:08:35.466Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:08:35.468Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:08:35.468Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [0e5a9c02c144] <==
* {"level":"info","ts":"2023-12-12T11:09:34.079Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-12-12T11:09:34.079Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-12-12T11:09:34.079Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-12-12T11:09:34.080Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-12-12T11:09:34.083Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-12-12T11:09:34.083Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-12-12T11:09:34.086Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.752585ms"}
{"level":"info","ts":"2023-12-12T11:09:34.757Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":30003,"snapshot-size":"7.5 kB"}
{"level":"info","ts":"2023-12-12T11:09:34.757Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":5476352,"backend-size":"5.5 MB","backend-size-in-use-bytes":5443584,"backend-size-in-use":"5.4 MB"}
{"level":"info","ts":"2023-12-12T11:09:35.456Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":39397}
{"level":"info","ts":"2023-12-12T11:09:35.464Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-12-12T11:09:35.465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 7"}
{"level":"info","ts":"2023-12-12T11:09:35.465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 7, commit: 39397, applied: 30003, lastindex: 39397, lastterm: 7]"}
{"level":"info","ts":"2023-12-12T11:09:35.466Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-12T11:09:35.466Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-12-12T11:09:35.466Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-12-12T11:09:35.475Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-12-12T11:09:35.480Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":30002}
{"level":"info","ts":"2023-12-12T11:09:35.489Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":31753}
{"level":"info","ts":"2023-12-12T11:09:35.493Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-12-12T11:09:35.495Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2023-12-12T11:09:35.547Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-12-12T11:09:35.548Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-12-12T11:09:35.549Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-12-12T11:09:35.554Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-12-12T11:09:35.556Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-12-12T11:09:35.555Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:09:35.555Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:09:35.557Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-12-12T11:09:35.557Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-12T11:09:35.557Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:09:35.557Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-12-12T11:09:36.368Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2023-12-12T11:09:36.369Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2023-12-12T11:09:36.373Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-12-12T11:09:36.373Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-12T11:09:36.373Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-12-12T11:09:36.373Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-12-12T11:09:36.373Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-12-12T11:09:36.375Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-12-12T11:09:36.375Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-12-12T11:09:44.348Z","caller":"traceutil/trace.go:171","msg":"trace[1608762459] transaction","detail":"{read_only:false; response_revision:31837; number_of_response:1; }","duration":"105.215155ms","start":"2023-12-12T11:09:44.243Z","end":"2023-12-12T11:09:44.348Z","steps":["trace[1608762459] 'process raft request'  (duration: 28.584919ms)","trace[1608762459] 'compare'  (duration: 76.352511ms)"],"step_count":2}

* 
* ==> kernel <==
*  11:11:00 up 1 day, 14:27,  0 users,  load average: 0.45, 0.52, 0.36
Linux minikube 5.15.49-linuxkit #1 SMP Tue Sep 13 07:51:46 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [9cd0629018a8] <==
* E1212 11:10:09.646797       1 storage.go:470] Address {10.244.0.28  0xc005142aa0 0xc000376460} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.646902       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc005142aa0 0xc000376460}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.656952       1 storage.go:470] Address {10.244.0.28  0xc004e42fe0 0xc000306f50} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.657008       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc004e42fe0 0xc000306f50}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.669906       1 storage.go:470] Address {10.244.0.28  0xc002cb4610 0xc000388cb0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.669963       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc002cb4610 0xc000388cb0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.686061       1 storage.go:470] Address {10.244.0.28  0xc002cb4f10 0xc000388d90} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.686116       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc002cb4f10 0xc000388d90}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.706318       1 storage.go:470] Address {10.244.0.28  0xc0051437a0 0xc000376fc0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.706442       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0051437a0 0xc000376fc0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.729061       1 storage.go:470] Address {10.244.0.28  0xc004f96730 0xc0003a85b0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.729130       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc004f96730 0xc0003a85b0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.748614       1 storage.go:470] Address {10.244.0.28  0xc004f96f80 0xc0003a8fc0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.748737       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc004f96f80 0xc0003a8fc0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.777508       1 storage.go:470] Address {10.244.0.28  0xc002fa02f0 0xc0001cf180} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.777788       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc002fa02f0 0xc0001cf180}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.844299       1 storage.go:470] Address {10.244.0.28  0xc002fa1f30 0xc000560460} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.844415       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc002fa1f30 0xc000560460}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:09.926162       1 storage.go:470] Address {10.244.0.28  0xc004f97e90 0xc00053e070} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:09.926273       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc004f97e90 0xc00053e070}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:10.027286       1 storage.go:470] Address {10.244.0.28  0xc0038acb80 0xc000560930} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:10.027351       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0038acb80 0xc000560930}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:10.170491       1 storage.go:470] Address {10.244.0.28  0xc0039119b0 0xc000560d20} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:10.170575       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0039119b0 0xc000560d20}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:10.493597       1 storage.go:470] Address {10.244.0.28  0xc00395d3d0 0xc0003b2c40} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:10.493891       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00395d3d0 0xc0003b2c40}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:10.993304       1 storage.go:470] Address {10.244.0.28  0xc00376e0b0 0xc0005cb8f0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:10.993565       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00376e0b0 0xc0005cb8f0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:11.753994       1 storage.go:470] Address {10.244.0.28  0xc003bcbab0 0xc00b22b2d0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:11.754363       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc003bcbab0 0xc00b22b2d0}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:12.191864       1 storage.go:470] Address {10.244.0.27  0xc0044bcbd0 0xc0027018f0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.54}] vs 10.244.0.27 (kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-c745v))
E1212 11:10:12.191955       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.27  0xc0044bcbd0 0xc0027018f0}] [] [{ 8000 TCP <nil>}]}
E1212 11:10:12.356274       1 storage.go:470] Address {10.244.0.28  0xc00429c4b0 0xc000599030} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:12.356319       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00429c4b0 0xc000599030}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:13.275573       1 storage.go:470] Address {10.244.0.28  0xc0040eeb50 0xc0005f0f50} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:13.275799       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0040eeb50 0xc0005f0f50}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:15.793045       1 storage.go:470] Address {10.244.0.28  0xc00549e8e0 0xc002784380} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:15.793159       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00549e8e0 0xc002784380}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:17.665654       1 storage.go:470] Address {10.244.0.28  0xc00543dd10 0xc002794930} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:17.665842       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00543dd10 0xc002794930}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:19.584341       1 storage.go:470] Address {10.244.0.28  0xc00553ace0 0xc00076de30} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:19.584452       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc00553ace0 0xc00076de30}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:27.459556       1 storage.go:470] Address {10.244.0.28  0xc0058a3da0 0xc003ea0700} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:27.459614       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0058a3da0 0xc003ea0700}] [] [{ 9090 TCP <nil>}]}
E1212 11:10:31.944636       1 storage.go:470] Address {10.244.0.28  0xc0062a0b50 0xc003f08150} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:31.944718       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc0062a0b50 0xc003f08150}] [] [{ 9090 TCP <nil>}]}
I1212 11:10:37.635528       1 handler_discovery.go:325] DiscoveryManager: Failed to download discovery for kube-system/metrics-server:443: 503 error trying to reach service: dial tcp 10.100.170.214:443: connect: connection refused
I1212 11:10:37.635579       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W1212 11:10:38.957503       1 handler_proxy.go:100] no RequestInfo found in the context
E1212 11:10:38.957538       1 controller.go:113] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: Error, could not get list of group versions for APIService
I1212 11:10:38.957543       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
W1212 11:10:38.958763       1 handler_proxy.go:100] no RequestInfo found in the context
E1212 11:10:38.958836       1 controller.go:116] loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
I1212 11:10:38.958884       1 controller.go:129] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I1212 11:10:39.359143       1 controller.go:624] quota admission added evaluator for: endpoints
E1212 11:10:42.199935       1 storage.go:470] Address {10.244.0.27  0xc0070b0af0 0xc00851a7e0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.54}] vs 10.244.0.27 (kubernetes-dashboard/dashboard-metrics-scraper-5dd9cbfd69-c745v))
E1212 11:10:42.199969       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.27  0xc0070b0af0 0xc00851a7e0}] [] [{ 8000 TCP <nil>}]}
E1212 11:10:50.365529       1 storage.go:470] Address {10.244.0.28  0xc007c0da80 0xc0097c80e0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.53}] vs 10.244.0.28 (kubernetes-dashboard/kubernetes-dashboard-5c5cfc8747-gfshs))
E1212 11:10:50.365568       1 storage.go:480] Failed to find a valid address, skipping subset: &{[{10.244.0.28  0xc007c0da80 0xc0097c80e0}] [] [{ 9090 TCP <nil>}]}

* 
* ==> kube-apiserver [cc71afe9c9fa] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.449760       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.452596       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.454112       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.455556       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.455556       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.455639       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1212 11:08:36.455655       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [1b3c411269a6] <==
* I1212 11:10:34.414240       1 serving.go:348] Generated self-signed cert in-memory
I1212 11:10:35.017155       1 controllermanager.go:187] "Starting" version="v1.27.4"
I1212 11:10:35.017208       1 controllermanager.go:189] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1212 11:10:35.018493       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1212 11:10:35.018729       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1212 11:10:35.019125       1 secure_serving.go:210] Serving securely on 127.0.0.1:10257
I1212 11:10:35.019224       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E1212 11:10:45.039753       1 controllermanager.go:233] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: Get \"https://192.168.49.2:8443/healthz\": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2"

* 
* ==> kube-proxy [778da19335de] <==
* I1212 11:09:40.381534       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1212 11:09:40.381810       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1212 11:09:40.381841       1 server_others.go:554] "Using iptables proxy"
I1212 11:09:40.677988       1 server_others.go:192] "Using iptables Proxier"
I1212 11:09:40.678017       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1212 11:09:40.678026       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1212 11:09:40.678035       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1212 11:09:40.678062       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1212 11:09:40.678564       1 server.go:658] "Version info" version="v1.27.4"
I1212 11:09:40.678575       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1212 11:09:40.681724       1 config.go:188] "Starting service config controller"
I1212 11:09:40.681753       1 shared_informer.go:311] Waiting for caches to sync for service config
I1212 11:09:40.681772       1 config.go:97] "Starting endpoint slice config controller"
I1212 11:09:40.681775       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1212 11:09:40.682047       1 config.go:315] "Starting node config controller"
I1212 11:09:40.682055       1 shared_informer.go:311] Waiting for caches to sync for node config
I1212 11:09:40.786869       1 shared_informer.go:318] Caches are synced for service config
I1212 11:09:40.786958       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1212 11:09:40.790740       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [ee71ad639290] <==
* I1212 11:01:31.697082       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1212 11:01:31.697155       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I1212 11:01:31.697177       1 server_others.go:554] "Using iptables proxy"
I1212 11:01:31.856973       1 server_others.go:192] "Using iptables Proxier"
I1212 11:01:31.857114       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1212 11:01:31.857145       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1212 11:01:31.857183       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1212 11:01:31.857244       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1212 11:01:31.859234       1 server.go:658] "Version info" version="v1.27.4"
I1212 11:01:31.859271       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1212 11:01:31.863303       1 config.go:188] "Starting service config controller"
I1212 11:01:31.863374       1 shared_informer.go:311] Waiting for caches to sync for service config
I1212 11:01:31.864300       1 config.go:97] "Starting endpoint slice config controller"
I1212 11:01:31.864330       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1212 11:01:31.865698       1 config.go:315] "Starting node config controller"
I1212 11:01:31.865843       1 shared_informer.go:311] Waiting for caches to sync for node config
I1212 11:01:31.971867       1 shared_informer.go:318] Caches are synced for node config
I1212 11:01:31.971906       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1212 11:01:31.971945       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [86cae000bb98] <==
* E1212 11:07:05.958719       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:15.298277       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:15.298405       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:17.945330       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:17.945454       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:21.546371       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:21.546524       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:21.955328       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:21.955474       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:22.394079       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:22.394225       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:23.445001       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:23.445147       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:25.736645       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:25.736862       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:31.253006       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:31.253071       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:34.121104       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:34.121144       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:34.728400       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:34.728527       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:36.157904       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:36.158160       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:39.662731       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:39.663241       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:46.545523       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:46.545852       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:51.031149       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:51.031336       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:51.086928       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:51.087020       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:07:59.256535       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:07:59.256657       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:01.725285       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:01.725383       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:06.106910       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:06.107039       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:06.338689       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:06.338919       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:10.879114       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:10.879574       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:11.158195       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:11.158480       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:11.955934       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:11.956090       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:13.841971       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:13.842124       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:15.026511       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:15.026608       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:20.874399       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:20.874494       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:23.563670       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:23.563771       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:25.516681       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:25.516740       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:08:26.525930       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:08:26.526028       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
I1212 11:08:35.378978       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1212 11:08:35.379357       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E1212 11:08:35.379465       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [c8c2f8bcea78] <==
* W1212 11:09:58.104796       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:09:58.104991       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:09:58.259270       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:09:58.259410       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:00.177774       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:00.177885       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:00.408247       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:00.408400       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:06.729470       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:06.729573       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:09.753196       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:09.753238       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:09.819409       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:09.819614       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:11.132554       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:11.132679       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:12.572832       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:12.572967       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:12.916993       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:12.917537       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:14.398565       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:14.399124       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:14.933863       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:14.933994       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:15.663530       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:15.663622       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:16.439877       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:16.440003       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:17.040019       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:17.040147       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:17.605540       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:17.605661       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:20.249000       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:20.249061       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:20.413026       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:20.413242       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:23.605471       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:23.605623       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:38.442386       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:38.442440       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:40.936349       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:40.936431       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:45.148696       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:45.149019       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:48.295558       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:48.295863       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:48.609841       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:48.609979       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:50.365923       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:50.366032       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:52.988668       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:52.988740       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:53.411229       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:53.411369       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:56.546452       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:56.546593       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:56.674635       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:56.674737       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
W1212 11:10:59.891966       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2
E1212 11:10:59.892021       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate is valid for 192.168.76.2, 10.96.0.1, 127.0.0.1, 10.0.0.1, not 192.168.49.2

* 
* ==> kubelet <==
* Dec 12 11:09:47 minikube kubelet[1485]: E1212 11:09:47.046586    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:09:48 minikube kubelet[1485]: I1212 11:09:48.082183    1485 scope.go:115] "RemoveContainer" containerID="e240c6594ba22666702edf3332a55f184de798f1bb359bd51b4779fe05f1a768"
Dec 12 11:09:48 minikube kubelet[1485]: I1212 11:09:48.082639    1485 scope.go:115] "RemoveContainer" containerID="df4e9a8aad8f38f7597f69f9bc80be19e13b49c6327d4f28683d73393eaf8719"
Dec 12 11:09:48 minikube kubelet[1485]: E1212 11:09:48.083094    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:09:52 minikube kubelet[1485]: I1212 11:09:52.145140    1485 scope.go:115] "RemoveContainer" containerID="df4e9a8aad8f38f7597f69f9bc80be19e13b49c6327d4f28683d73393eaf8719"
Dec 12 11:09:52 minikube kubelet[1485]: E1212 11:09:52.145653    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:09:57 minikube kubelet[1485]: I1212 11:09:57.951908    1485 scope.go:115] "RemoveContainer" containerID="df4e9a8aad8f38f7597f69f9bc80be19e13b49c6327d4f28683d73393eaf8719"
Dec 12 11:09:58 minikube kubelet[1485]: I1212 11:09:58.766500    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:09:59 minikube kubelet[1485]: I1212 11:09:59.766160    1485 scope.go:115] "RemoveContainer" containerID="a936c51fbb89beb1fca17e7b32baed534a719c473526aba1393d91fe3e34c206"
Dec 12 11:10:00 minikube kubelet[1485]: E1212 11:10:00.551298    1485 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:00 minikube kubelet[1485]: E1212 11:10:00.551447    1485 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:00 minikube kubelet[1485]: E1212 11:10:00.551819    1485 kuberuntime_manager.go:1212] container &Container{Name:flask-app-container,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llndt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod flask-deployment-fd66fcbf4-w4pwj_default(ffe027a2-90f3-4378-8fce-c749f733c805): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 12 11:10:00 minikube kubelet[1485]: E1212 11:10:00.552132    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:10:02 minikube kubelet[1485]: E1212 11:10:02.334421    1485 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:02 minikube kubelet[1485]: E1212 11:10:02.334599    1485 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:02 minikube kubelet[1485]: E1212 11:10:02.334826    1485 kuberuntime_manager.go:1212] container &Container{Name:flask-app-container,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfkkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod flask-deployment-fd66fcbf4-2m6p9_default(247c29a4-94b9-4853-bdc9-7b56675fa981): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 12 11:10:02 minikube kubelet[1485]: E1212 11:10:02.334896    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-fd66fcbf4-2m6p9" podUID=247c29a4-94b9-4853-bdc9-7b56675fa981
Dec 12 11:10:09 minikube kubelet[1485]: I1212 11:10:09.500070    1485 scope.go:115] "RemoveContainer" containerID="df4e9a8aad8f38f7597f69f9bc80be19e13b49c6327d4f28683d73393eaf8719"
Dec 12 11:10:09 minikube kubelet[1485]: I1212 11:10:09.500399    1485 scope.go:115] "RemoveContainer" containerID="44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23"
Dec 12 11:10:09 minikube kubelet[1485]: E1212 11:10:09.500954    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:10 minikube kubelet[1485]: I1212 11:10:10.528941    1485 scope.go:115] "RemoveContainer" containerID="a8935e465df5e051a0db9c19f16e9e9e663910a06429d63018ba567a46c191f7"
Dec 12 11:10:10 minikube kubelet[1485]: I1212 11:10:10.529270    1485 scope.go:115] "RemoveContainer" containerID="b951b1fd7b339e19ea0210db9a822e6a95b9f433a384268e804787302d5f99f3"
Dec 12 11:10:10 minikube kubelet[1485]: E1212 11:10:10.529479    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(1125b31a-0ff3-4f5f-b242-15669a3d68db)\"" pod="kube-system/storage-provisioner" podUID=1125b31a-0ff3-4f5f-b242-15669a3d68db
Dec 12 11:10:11 minikube kubelet[1485]: I1212 11:10:11.765162    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:10:11 minikube kubelet[1485]: E1212 11:10:11.769469    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:10:12 minikube kubelet[1485]: I1212 11:10:12.145278    1485 scope.go:115] "RemoveContainer" containerID="44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23"
Dec 12 11:10:12 minikube kubelet[1485]: E1212 11:10:12.146068    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:13 minikube kubelet[1485]: I1212 11:10:13.765232    1485 scope.go:115] "RemoveContainer" containerID="a936c51fbb89beb1fca17e7b32baed534a719c473526aba1393d91fe3e34c206"
Dec 12 11:10:13 minikube kubelet[1485]: E1212 11:10:13.770813    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-2m6p9" podUID=247c29a4-94b9-4853-bdc9-7b56675fa981
Dec 12 11:10:17 minikube kubelet[1485]: I1212 11:10:17.952141    1485 scope.go:115] "RemoveContainer" containerID="44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23"
Dec 12 11:10:17 minikube kubelet[1485]: E1212 11:10:17.953066    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:21 minikube kubelet[1485]: I1212 11:10:21.765412    1485 scope.go:115] "RemoveContainer" containerID="b951b1fd7b339e19ea0210db9a822e6a95b9f433a384268e804787302d5f99f3"
Dec 12 11:10:23 minikube kubelet[1485]: I1212 11:10:23.766241    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:10:25 minikube kubelet[1485]: E1212 11:10:25.538625    1485 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:25 minikube kubelet[1485]: E1212 11:10:25.538737    1485 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:25 minikube kubelet[1485]: E1212 11:10:25.538856    1485 kuberuntime_manager.go:1212] container &Container{Name:flask-app-container,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-llndt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod flask-deployment-fd66fcbf4-w4pwj_default(ffe027a2-90f3-4378-8fce-c749f733c805): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 12 11:10:25 minikube kubelet[1485]: E1212 11:10:25.538891    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:10:26 minikube kubelet[1485]: I1212 11:10:26.764391    1485 scope.go:115] "RemoveContainer" containerID="a936c51fbb89beb1fca17e7b32baed534a719c473526aba1393d91fe3e34c206"
Dec 12 11:10:28 minikube kubelet[1485]: E1212 11:10:28.560096    1485 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:28 minikube kubelet[1485]: E1212 11:10:28.560173    1485 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-flask-app:latest"
Dec 12 11:10:28 minikube kubelet[1485]: E1212 11:10:28.560297    1485 kuberuntime_manager.go:1212] container &Container{Name:flask-app-container,Image:my-flask-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{50 -3} {<nil>} 50m DecimalSI},memory: {{134217728 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfkkv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod flask-deployment-fd66fcbf4-2m6p9_default(247c29a4-94b9-4853-bdc9-7b56675fa981): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 12 11:10:28 minikube kubelet[1485]: E1212 11:10:28.560332    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-flask-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flask-deployment-fd66fcbf4-2m6p9" podUID=247c29a4-94b9-4853-bdc9-7b56675fa981
Dec 12 11:10:33 minikube kubelet[1485]: I1212 11:10:33.764206    1485 scope.go:115] "RemoveContainer" containerID="44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23"
Dec 12 11:10:35 minikube kubelet[1485]: I1212 11:10:35.766113    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:10:35 minikube kubelet[1485]: E1212 11:10:35.770243    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:10:42 minikube kubelet[1485]: I1212 11:10:42.766460    1485 scope.go:115] "RemoveContainer" containerID="a936c51fbb89beb1fca17e7b32baed534a719c473526aba1393d91fe3e34c206"
Dec 12 11:10:42 minikube kubelet[1485]: E1212 11:10:42.772987    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-2m6p9" podUID=247c29a4-94b9-4853-bdc9-7b56675fa981
Dec 12 11:10:45 minikube kubelet[1485]: I1212 11:10:45.243291    1485 scope.go:115] "RemoveContainer" containerID="44d827ca5c7f3650fb3e561f155b407c037292566811a7d9dc422e8275c51a23"
Dec 12 11:10:45 minikube kubelet[1485]: I1212 11:10:45.243677    1485 scope.go:115] "RemoveContainer" containerID="1b3c411269a62d76d03d8a766bb75501334dd17c87781376d960d3cb697a0167"
Dec 12 11:10:45 minikube kubelet[1485]: E1212 11:10:45.244140    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:47 minikube kubelet[1485]: I1212 11:10:47.766126    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:10:47 minikube kubelet[1485]: E1212 11:10:47.770465    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805
Dec 12 11:10:47 minikube kubelet[1485]: I1212 11:10:47.953189    1485 scope.go:115] "RemoveContainer" containerID="1b3c411269a62d76d03d8a766bb75501334dd17c87781376d960d3cb697a0167"
Dec 12 11:10:47 minikube kubelet[1485]: E1212 11:10:47.954678    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:52 minikube kubelet[1485]: I1212 11:10:52.145298    1485 scope.go:115] "RemoveContainer" containerID="1b3c411269a62d76d03d8a766bb75501334dd17c87781376d960d3cb697a0167"
Dec 12 11:10:52 minikube kubelet[1485]: E1212 11:10:52.146043    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(b3702ceb912504d37098b922ccdcfa41)\"" pod="kube-system/kube-controller-manager-minikube" podUID=b3702ceb912504d37098b922ccdcfa41
Dec 12 11:10:54 minikube kubelet[1485]: I1212 11:10:54.766208    1485 scope.go:115] "RemoveContainer" containerID="a936c51fbb89beb1fca17e7b32baed534a719c473526aba1393d91fe3e34c206"
Dec 12 11:10:54 minikube kubelet[1485]: E1212 11:10:54.772105    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-2m6p9" podUID=247c29a4-94b9-4853-bdc9-7b56675fa981
Dec 12 11:11:00 minikube kubelet[1485]: I1212 11:11:00.767241    1485 scope.go:115] "RemoveContainer" containerID="d93600190b808fba198fe7d7854401195e69fa394c65edb91ff0dd19601be1e9"
Dec 12 11:11:00 minikube kubelet[1485]: E1212 11:11:00.776119    1485 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flask-app-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-flask-app:latest\\\"\"" pod="default/flask-deployment-fd66fcbf4-w4pwj" podUID=ffe027a2-90f3-4378-8fce-c749f733c805

* 
* ==> kubernetes-dashboard [3e1a4b1ade84] <==
* 2023/12/12 11:09:41 Using namespace: kubernetes-dashboard
2023/12/12 11:09:41 Using in-cluster config to connect to apiserver
2023/12/12 11:09:41 Using secret token for csrf signing
2023/12/12 11:09:41 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/12/12 11:09:41 Successful initial request to the apiserver, version: v1.27.4
2023/12/12 11:09:41 Generating JWE encryption key
2023/12/12 11:09:41 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/12/12 11:09:41 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/12/12 11:09:42 Initializing JWE encryption key from synchronized object
2023/12/12 11:09:42 Creating in-cluster Sidecar client
2023/12/12 11:09:42 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:09:42 Serving insecurely on HTTP port: 9090
2023/12/12 11:10:12 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:10:42 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:09:41 Starting overwatch

* 
* ==> kubernetes-dashboard [b193cb8eb2a6] <==
* 2023/12/12 11:01:50 Using namespace: kubernetes-dashboard
2023/12/12 11:01:50 Using in-cluster config to connect to apiserver
2023/12/12 11:01:50 Using secret token for csrf signing
2023/12/12 11:01:50 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/12/12 11:01:50 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/12/12 11:01:50 Successful initial request to the apiserver, version: v1.27.4
2023/12/12 11:01:50 Generating JWE encryption key
2023/12/12 11:01:50 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/12/12 11:01:50 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/12/12 11:01:50 Initializing JWE encryption key from synchronized object
2023/12/12 11:01:50 Creating in-cluster Sidecar client
2023/12/12 11:01:50 Serving insecurely on HTTP port: 9090
2023/12/12 11:01:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:02:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:02:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:03:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:03:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:04:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:04:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:05:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:05:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:06:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:06:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:07:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:07:50 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:08:20 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/12/12 11:01:50 Starting overwatch

* 
* ==> storage-provisioner [4c33ac650a2e] <==
* I1212 11:10:21.898193       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1212 11:10:21.910170       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1212 11:10:21.910491       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1212 11:10:39.367213       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1212 11:10:39.367981       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8a730277-ed0f-4212-a0d0-4831a35e7144!
I1212 11:10:39.368980       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e999abc5-7c5f-4c1a-9563-ad7fb6b7e587", APIVersion:"v1", ResourceVersion:"31938", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8a730277-ed0f-4212-a0d0-4831a35e7144 became leader
I1212 11:10:39.468474       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8a730277-ed0f-4212-a0d0-4831a35e7144!

* 
* ==> storage-provisioner [b951b1fd7b33] <==
* I1212 11:09:40.277662       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1212 11:10:10.300608       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

